{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processed Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will process the data to normalize it before importing it into the database while preserving the relationships between the tables. The tables will be named CardioTrainNormalize, GlucoseTypes, CholesterolTypes and we will make all necessary changes to ensure proper data transformation.\n",
    "\n",
    "Ensure that you already have your own .env file containing your environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "work_dir = os.getenv('WORK_DIR')\n",
    "sys.path.append(work_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libraries & Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.models import CardioTrainNormalize, GlucoseTypes, CholesterolTypes, CauseOfDeaths\n",
    "from src.database.dbconnection import getconnection\n",
    "from sqlalchemy import inspect\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "from transform.TransformData import DataTransform, DataTransformCauseOfDeaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the SQLAlchemy library, connect to the database. If you encounter any issues, check that your .env file contains the correct environment variables and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conected successfully to database airflow_project!\n"
     ]
    }
   ],
   "source": [
    "engine = getconnection()\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to create the categories tables first, as it serves as the foreign key for the cardioTrain table. This will help avoid any potential errors. In this process, ensure that there are no other tables with the same name. If such tables exist, they should be dropped before creating the new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if inspect(engine).has_table('CauseOfDeaths'):\n",
    "        CauseOfDeaths.__table__.drop(engine, checkfirst=True)\n",
    "    CauseOfDeaths.__table__.create(engine)\n",
    "    print(\"Table created successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if inspect(engine).has_table('GlucoseTypes'):\n",
    "        if inspect(engine).has_table('CardioTrainNormalize'):\n",
    "            CardioTrainNormalize.__table__.drop(engine)\n",
    "        GlucoseTypes.__table__.drop(engine, checkfirst=True)\n",
    "    GlucoseTypes.__table__.create(engine)\n",
    "    print(\"Table created successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if inspect(engine).has_table('CholesterolTypes'):\n",
    "        if inspect(engine).has_table('CardioTrainNormalize'):\n",
    "            CardioTrainNormalize.__table__.drop(engine)\n",
    "        CholesterolTypes.__table__.drop(engine, checkfirst=True)\n",
    "    CholesterolTypes.__table__.create(engine)\n",
    "    print(\"Table created successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if inspect(engine).has_table('CardioTrainNormalize'):\n",
    "        CardioTrainNormalize.__table__.drop(engine)\n",
    "    CardioTrainNormalize.__table__.create(engine)\n",
    "    print(\"Table created successfully.\")\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Error creating table: {e}\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations on cardio_train.csv:\n",
    "\n",
    "1. Gender Categorization: The gender column was mapped from numeric values to categorical labels ('Female' and 'Male').\n",
    "\n",
    "2. Cholesterol Categorization: The cholesterol column was converted from numeric levels to categories like 'normal', 'above normal', and 'well above normal'.\n",
    "\n",
    "3. Glucose Categorization: Similar to cholesterol, the gluc column was categorized into 'normal', 'above normal', and 'well above normal'.\n",
    "\n",
    "4. BMI Calculation: The Body Mass Index (BMI) was calculated using the formula: BMI = weight / (height in meters)^2. The result was stored in a new bmi column.\n",
    "\n",
    "5. Age Calculation : The age column, initially recorded in days, was converted to years by dividing by 365.3 and taking the floor value.\n",
    "\n",
    "6. Blood Pressure Standardization :\n",
    "\n",
    "- Absolute values of ap_hi (systolic) and ap_lo (diastolic) were taken.\n",
    "- Records with systolic blood pressure below 80 or above 250, and diastolic below 50 or above 150, were removed.\n",
    "- Rows where systolic pressure equaled diastolic pressure were also removed.\n",
    "\n",
    "7. BMI Categorization (CategorizeBMI): The bmi column was categorized into different classes based on predefined BMI ranges, and the results were stored in a new bmi_class column.\n",
    "\n",
    "8. Blood Pressure Categorization (categorize_blood_pressure): Blood pressure readings were categorized into different levels based on standard hypertension guidelines, and the results were stored in a bp_cat column.\n",
    "\n",
    "9. Pulse Pressure Calculation (CalculatePulsePressure): A new column pulse_press was created to store the difference between systolic (ap_hi) and diastolic (ap_lo) blood pressure, known as pulse pressure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization of Glucose and Cholesterol Levels:\n",
    "\n",
    "1. Glucose Normalization (nomalize_gluc): Unique glucose levels were extracted and mapped to new IDs. The original gluc column was replaced with these IDs, and the glucose types were stored in a separate table called GlucoseTypes.\n",
    "2. Cholesterol Normalization (normalize_cholesterol): Similar to glucose, cholesterol levels were normalized and stored in a CholesterolTypes table, with the original cholesterol column being replaced by corresponding IDs.\n",
    "3. Transformations on cause_of_deaths.csv:\n",
    "-  ID Insertion: A new id column was created to uniquely identify each row.\n",
    "\n",
    "- Column Dropping: The Code column was removed from the dataset.\n",
    "\n",
    "4. Total Deaths Calculation : A TotalDeaths column was added, summing up all death counts across the specified columns. The dataset was then reorganized to display this new column alongside specific causes of death, like Cardiovascular.\n",
    "\n",
    "These transformations were designed to clean, standardize, and normalize the data for further analysis and storage in an SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Choicelist and default value do not have a common dtype: The DType <class 'numpy.dtypes._PyLongDType'> could not be promoted by <class 'numpy.dtypes.StrDType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes.StrDType'>, <class 'numpy.dtypes._PyLongDType'>)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    #Cardio train transform\n",
    "    file = DataTransform('../../data/cardio_train.csv')\n",
    "    file.gender_by_category()\n",
    "    file.cholesterol_by_category()\n",
    "    file.gluc_by_category()\n",
    "    file.bmi()\n",
    "    file.days_to_age()\n",
    "    file.StandardizeBloodPressure()\n",
    "    file.CategorizeBMI()\n",
    "    file.categorize_blood_pressure()\n",
    "    file.CalculatePulsePressure()\n",
    "\n",
    "    file.df.to_sql('CardioTrainNormalize', con=engine, if_exists='append', index=False)\n",
    "\n",
    "    #Cause Of Deaths transform\n",
    "    file2 = DataTransformCauseOfDeaths('../../data/cause_of_deaths.csv')\n",
    "    file2.total_deaths()\n",
    "    file2.insert_id()\n",
    "\n",
    "    file2.df.to_sql('CauseOfDeaths', con=engine, if_exists='append', index=False)\n",
    "\n",
    "\n",
    "    print(\"Data uploaded\")\n",
    "\n",
    "except SQLAlchemyError as e:\n",
    "    print(f\"Database error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if hasattr(engine, 'dispose'):\n",
    "        engine.dispose()\n",
    "    if 'session' in locals():\n",
    "        session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transformed health data successfully stored in our database, we are now poised to extract new data from the API. This next phase will allow us to gather relevant information that will further enrich our analysis.\n",
    "\n",
    "Now you can proceed with the next notebook: [004_API.ipynb](../004_API.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
